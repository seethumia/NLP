{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seethu-8363/Documents/Test/virenv1/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer , TfidfTransformer\n",
    "from sklearn import decomposition, ensemble\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class naive_bayes_sentiment:\n",
    "    def data_preproc(self):\n",
    "        \n",
    "        #Loading the data\n",
    "        data = open('/Users/seethu-8363/Downloads/amazon_reviews.csv').read()\n",
    "        \n",
    "        labels, texts = [], []\n",
    "        for i, line in enumerate(data.split(\"\\n\")):\n",
    "            content = line.split()\n",
    "            labels.append(content[0])\n",
    "            texts.append(\" \".join(content[1:]))\n",
    "        # create a dataframe using texts and lables\n",
    "        trainDF = pd.DataFrame()\n",
    "        trainDF['text'] = texts\n",
    "        trainDF['label'] = labels\n",
    "        \n",
    "        #lower case\n",
    "        trainDF[\"text\"] = trainDF[\"text\"].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "        trainDF[\"text\"].head()\n",
    "        \n",
    "        #STOP WORDS\n",
    "        #Removal of stop words\n",
    "        stop = stopwords.words('english')\n",
    "        ##Creating a list of custom stopwords\n",
    "        new_words = [\"using\", \"show\", \"result\", \"large\", \"also\", \"iv\", \"one\", \"two\", \"new\", \"previously\", \"shown\",\"way\",\"ever\",\n",
    "             \"us\",\"up\",\"cd\",\"that\",\"all\",\"second\"]\n",
    "        stop.append(new_words)\n",
    "        trainDF[\"text\"] = trainDF[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "        \n",
    "        \n",
    "        #Removal of punctautaion \n",
    "        trainDF[\"text\"] = trainDF[\"text\"].str.replace('[^\\w\\s]','  ')\n",
    "\n",
    "        \n",
    "        # split the dataset into training and validation datasets \n",
    "        train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'],test_size = 0.8)\n",
    "        \n",
    "        # label encode the target variable \n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        train_y = encoder.fit_transform(train_y)\n",
    "        valid_y = encoder.fit_transform(valid_y)\n",
    "        \n",
    "    \n",
    "        return train_x, train_y, valid_x, valid_y\n",
    "        \n",
    "    def train_model_pipeline(self, classifier, train_x, label , valid_x , valid_y):\n",
    "             # fit the training dataset on the classifier\n",
    "            \n",
    "            tfidf_vectorizer = TfidfVectorizer()\n",
    "            nb = classifier\n",
    "            #creating pipeline for model mand vectoriser\n",
    "            tfidf_nv_pipe = Pipeline([('tfidf', tfidf_vectorizer), ('nb', nb)])\n",
    "\n",
    "            #Fitting the data\n",
    "            tfidf_nv_pipe.fit(train_x,label)\n",
    "    \n",
    "            # predict the labels on validation dataset\n",
    "            predictions = tfidf_nv_pipe.predict(valid_x)\n",
    "    \n",
    "            return metrics.f1_score(predictions, valid_y , average = 'micro'),tfidf_nv_pipe \n",
    "    \n",
    "    def load_svm_model(self, train_x , train_y , valid_x , valid_y):\n",
    "            \n",
    "       # SVM RBC on Word Level TF IDF Vectors with Grid Search\n",
    "        accuracy,model = self.train_model_pipeline(svm.SVC(kernel = 'rbf'), train_x, train_y, valid_x , valid_y) \n",
    "        return accuracy,model\n",
    "        \n",
    "    def prediction_svm(self,input_str):\n",
    "            train_x , train_y , valid_x , valid_y = self.data_preproc()\n",
    "            accuracy = self.load_svm_model(train_x , train_y , valid_x , valid_y)\n",
    "            print(\"NB, WordLevel TF-IDF ACCURACY: \", accuracy)\n",
    "            #loaded_model = joblib.load(\"/Users/seethu-8363/Documents/Test/virenv1/model/sentiment_analysis_naive_bayes_model.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seethu-8363/Documents/Test/virenv1/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, WordLevel TF-IDF ACCURACY:  (0.507625, Pipeline(memory=None,\n",
      "         steps=[('tfidf',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=1.0, max_features=None,\n",
      "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('nb',\n",
      "                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "                     decision_function_shape='ovr', degree=3,\n",
      "                     gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
      "                     probability=False, random_state=None, shrinking=True,\n",
      "                     tol=0.001, verbose=False))],\n",
      "         verbose=False))\n"
     ]
    }
   ],
   "source": [
    "#SVC\n",
    "obj = naive_bayes_sentiment()\n",
    "obj.prediction_svm('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    9.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 1}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the data\n",
    "data = open('/Users/seethu-8363/Downloads/amazon_reviews.csv').read()\n",
    "        \n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "\n",
    "# create a dataframe using texts and lables\n",
    "trainDF = pd.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels\n",
    "\n",
    "\n",
    "\n",
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'],test_size = 0.8)\n",
    "\n",
    "\n",
    "# count_vect = CountVectorizer()\n",
    "# X_train_counts = count_vect.fit_transform(train_x)\n",
    "# tfidf_transformer = TfidfTransformer()\n",
    "# X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "# clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "# label encode the target variable \n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "        \n",
    "#vectorizer\n",
    "\n",
    "#Fit the vectorizer on train data and tranform the vectorizer on train and test\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(train_x)\n",
    "vec_train_data = tfidf_vectorizer.transform(train_x)\n",
    "vec_test_data = tfidf_vectorizer.transform(valid_x)\n",
    "\n",
    "\n",
    "model = svm.SVC(kernel = 'rbf')\n",
    "\n",
    "#Making a grid based on the values provided by Random Search\n",
    "param_grid = {\n",
    "    'C' : [0.001, 0.01, 0.1, 1, 10],\n",
    "    'gamma' : [0.001, 0.01, 0.1, 1]\n",
    "            }\n",
    "        \n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = model, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2,scoring = 'f1')\n",
    "\n",
    "grid_search.fit(vec_train_data,train_y)\n",
    "grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8399846015655076\n"
     ]
    }
   ],
   "source": [
    "model = svm.SVC(C = 10 , kernel = 'rbf',gamma= 1)\n",
    "model.fit(vec_train_data,train_y)\n",
    "prediction = model.predict(vec_test_data)\n",
    "print(metrics.f1_score(prediction, valid_y))\n",
    "\n",
    "#SVM Model\n",
    "#accuarcy before grid search = 0.48575\n",
    "#accuracy after grid search = 0.8399846015655076"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 13675)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 13675)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['C', 'cache_size', 'class_weight', 'coef0', 'decision_function_shape', 'degree', 'gamma', 'kernel', 'max_iter', 'probability', 'random_state', 'shrinking', 'tol', 'verbose'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = svm.SVC(kernel = 'rbf')\n",
    "svm_model.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'accuracy', 'roc_auc', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'brier_score_loss', 'adjusted_rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a grid based on the values provided by Random Search\n",
    "\n",
    "param_grid = {\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    gammas = [0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "#Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_\n",
    "\n",
    "\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "gammas = [0.001, 0.01, 0.1, 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
